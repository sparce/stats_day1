---
title: "Stats in R - Day 1"
output: html_notebook
---

```{r loading_libraries, include=FALSE}
library(tidyverse)
```


# Exercise 1E - Sampling and distributions

Sampling from distributions in R is done using the `r<distrib>()` functions, where <distrib> is 
substituted for the short name of a distribution. Some distributions you might be exposed to 
(and their sampling functions) are:

* The Normal (Gaussian) distribution - `rnorm()`
* The Poisson distribution - `rpois()`
* The Gamma distribution - `rgamma()`
* The Exponential distribution - `rexp()`
* The Binomial distribution - `rbinom()`

Each of these functions are called with an argument `n` for the number of samples to take, as well 
as arguments that specify the parameters of the distribution. For example, a normal distribution is
defined by it's mean and standard deviation, so these can be provided to `rnorm()`

```{r distribution_sampling}
#Using default mean (0) and standard deviation (1)
normal_default <- rnorm(n = 10)

#Setting mean and standard deviation
normal_specified <- rnorm(n = 10, mean = 5, sd = 3)

normal_default

normal_specified
```

Using these sampling functions, complete the following exercises. Some initial code has been 
provided but you will need to fill in the blanks (????? or _____) and extend them to other 
distributions.

>Sample from the Normal, Binomial and one other distribution.  Take 5, 10, 200 and 1000 samples 
with default parameters and histogram the results.   

```{r changing_sample_size}
#Normal distribution
norm_5 <- rnorm(n = 5)
norm_10 <- rnorm(n = 10)
norm_200 <- rnorm(_____)
_____

#Histograms
hist(norm_5)
hist(norm_10)
_____
_____
```

> Take 30 samples and repeat this 6 times for your chosen distributions. Histogram the observations.
How does the histogram of the samples look different each time?

```{r repeated_sampling}
#Normal distribution
repeated_norm <- tibble(
  a = rnorm(30), 
  b = rnorm(30), 
  c = rnorm(30), 
  d = rnorm(30), 
  e = rnorm(30), 
  f = rnorm(30)
)

repeated_norm

#Histograms with hist()
hist(repeated_norm$a)
hist(repeated_norm$b)
_____

#Histograms with ggplot
long_norm <- gather(repeated_norm, sample_id, sample_value)

long_norm

ggplot(long_norm, aes(x = sample_value)) + geom_histogram() + facet_wrap(~ sample_id)
```

> Create box and whisker plots for your repeated samples. The central box shows between (roughly) 
the quartiles of the data, with the median represented  by a line. The 'whiskers' goes out to the 
extremes of data. Compare with the histograms, do they summarise the distributions well?

By default, the `boxplot()` function will plot one boxplot for each column in your data. This may 
not be suitable if you have a tidy dataframe with one column specifying a grouping variable. In this
case, you can use the formula method to create boxplots of one column based on groupings in a second
column. Both examples are shown below with our wide and long datasets.

Using `ggplot` provides more control over how the boxplots are constructed. But it may be overkill 
if you are just wanting a quick initial look at your data.

```{r boxplots}
#Normal distribution
##Boxplots with boxplot()
boxplot(repeated_norm)

boxplot(sample_value ~ sample_id, data = long_norm)

#Boxplots with ggplot
ggplot(long_norm, aes(x=sample_id, y = sample_value)) + geom_?????()
```

> To practice these on 'real' data, we will use the inbuilt datasets `sleep` and `InsectSprays`. 
Explore these two datasets with histograms and boxplots. What are the questions each dataset is 
trying to answer, and what are your initial impressions from looking at the data?

```{r real_data}
#Learn about the datasets
?sleep
?InsectSprays

#Histograms


#Boxplots

```

# Exercise 3E - Summary statistics and test statistics

>Using previous datasets, use R to estimate summary statistics for each group in the data.  

R has a number of inbuilt functions to calculate summary statistics. Each of them takes a set of 
numbers and reduces them down to a single summary value. For this exercise, we will be asking you
to calculate the:

* Mean - `mean()`
* Median - `median()`
* Variance - `var()`
* Standard deviation - `sd()`
* Interquartile range - `IQR()`

for the two datasets used previously.

```{r summary_stats}
#Summarise sleep
sleep %>% 
  group_by(_____) %>% 
  summarise(_____)

#Summarise InsectSprays
InsectSprays %>% 
  _____
```

>Apply a t-test to compare means between two samples.

To ask if the means between two samples are the same, you can use the `t.test()` function. This uses
the same formula notation as the boxplots did earlier. Remember that the structure of this formula 
is the response variable (what you are measuring) on the left, the grouping variable on the right.

Since the InsectSprays dataset has several groups, you will need to filter out just two to perform 
the t test otherwise you will get an error. Pick a few pairs of sprays to compare (from the boxplot
or summary statistics). 

```{r t_tests}
# sleep t test
sleep_t <- t.test(_____ ~ _____, data = sleep)

sleep_t

# InsectSprays t test
spray_AB <- filter(InsectSprays, _____)

spray_AB_t <- t.test(_____)

spray_AB_t

```

What information does the t test show you, and how do you interpret these result?

# Exercise 6A v) - Linear regression

> Apply linear least-squares regression to the `iris` dataset, and create scatter plots.

### About the iris data
From `?iris`: The iris data set has measurements (in cm)  of the variables sepal length
and width and petal length and width, respectively,for 50 flowers from
each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.

We will walk you through performing this analysis for the petal length and width data, and then you 
can repeat it for the sepal length and width.

```{r data_exploration}
#Use just the petal data from the iris dataset
petal <- iris %>% 
  select(Petal.Width, Petal.Length) %>% 
  rename(p_width = Petal.Width, p_length = Petal.Length)

petal

#Look at the data before we start to model it
petal_plot <- ggplot(petal, aes(x = p_width, y = p_length)) +
  geom_point()

petal_plot
```

It appears as though that petal length increases with petal width. We can build a linear regression
that models this relationship using the `lm` command. This function also uses the formula notation
that you have used several times already today.

```{r linear_regression}
petal_model <- lm(p_length ~ p_width, data = petal)

#Short summary of the model
petal_model

#Longer summary of the model
summary(petal_model)
```
Just printing the linear model out provides a basic look at the results. Remember that the equation
for a line is $Y = aX + b$. In fitting the linear model with `lm`, we are trying to determine the 
"best" values of `a` and `b` that minimise the amount of error between the straight line and our data
points. 

The output from the model is telling us that the best estimate for `a` is 2.230 and for `b` is 1.084

The more detailed output from `summary(petal_model)` gives additional information that can be used 
to validate and interpret our model. 

We can add the result from our model on top of our initial plot to see the relationship:

```{r regression_results}
#Manually
petal_plot + 
  geom_abline(intercept = 1.084, slope = 2.230)

#Using geom_smooth()
petal_plot +
  geom_smooth(method = "lm", se = FALSE)
```

Try it yourself by modelling the relationship between the sepal length and petal width data from 
`iris`. What relationship do you find?

```{r sepal_regression}
#Get data
sepal <- iris %>% 
  select(_____) %>% 
  rename(_____)

#Preliminary exploratory plots
sepal_plot <- ggplot(_____) + 
  _____

#Model relationship using lm
sepal_model <- lm(_____ ~ _____, data = sepal)

#Check model
sepal_model

#Overlay model results on initial plot
sepal_plot +
  _____
```

> Look at the residuals of this regression to assess it

You can get some summary plots of your model (including the residuals), by calling `plot` on the 
result of running `lm`. Alternatively, if you install the `ggResidpanel` package you can produce
diagnostic plots using `resid_panel`.

The diagnostic plots from `resid_panel` show:
*  Residual plot -residuals vs predicted values - test constant variance
*  QQ plot - tests normality assumption
*  Index Plot - additional plot to test assumption
*  Histogram of residues - check if any skewness is present

```{r diagnostic_plots}
library(ggResidpanel)

resid_panel(petal_model)
```

What do the diagnostic plots for your linear regression on the sepal measurements look like?

```{r sepal_diagnostics}

```

> Use your model to make predictions on new data

Ultimately, we wish to use the model to make some predictions about what may occur with new samples. 

We know that our petal model has the form: p_length = 1.084 + 2.230 * p_width
and we may wish to know what is the predicted length when then width is 0.75 cm.
One way to do this is manually calculate it using the command line:

```{r manual_prediction}
1.084 + 2.230 * 0.75
```

But this is obviously not going to work for more than a few predictions. Instead, we can use the
`predict` function in R. This function takes a fitted model and a data frame containing new data and
uses the model to predict the outcome.

For example, to predict the single case above where petal width is 0.75:

```{r single_predict}
#New data frame with same explanatory variable columns as our model
new_data <- tibble(p_width = 0.75)

predict(petal_model, new_data)
```

But `predict` allows us to predict many more values by providing a larger data frame of new data

```{r more_predictions}
# You can predict two values
two_new_data <- tibble(p_width = c(0.05, 0.75))

predict(petal_model, two_new_data)

# Or you can predict for a range of values (0.60 0.65 0.70 0.75 0.80 0.85 0.90)
many_new_data <- tibble(p_width = seq(0.6, 0.9, 0.05))

predict(petal_model, many_new_data)
```

# Exercise 6B iii) - Linear regression assumptions

>We'll create perfect linear data, and linear data with normal (Gaussian)
random noise, and non-normal random noise, data with outliers,
and nonlinear data.

Let's start with the basic relationship y = 2 * x + 5 and look at that over the values from 1 to 10.
We'll then add some noise to this data by sampling or change it from a linear relationship.

```{r data_setup}
x = seq(1, 10, by = 0.5)
perfect_linear <- (2 * x) + 5

test_data <- tibble(x, perfect_linear) %>% 
  mutate(
    #Normal noise
    normal_noise = perfect_linear + rnorm(n = length(x), mean = 0, sd = 2),
    
    #Non-normal noise: use exponential distribution
    non_normal_noise = perfect_linear + rexp(n = length(x)),
    
    #Outliers: normal noise plus 10% chance of adding 8 to the value
    outliers = perfect_linear + 
      rnorm(n = length(x)) + 
      sample(c(0, 8), prob = c(0.9, 0.1), size = length(x), replace = T),
    
    #Non-linear data: make the relationship quadratic instead
    non_linear = x^2 + 5
  )

test_data
```

>  Fit a linear model for each of these situations and produce diagnostic plots. Note how 
they look when the assumptions of linear regression are violated.

As a reminder, we will demonstrate these steps using the perfect linear data.

```{r perfect_linear_example}
#Exploratory plotting
ggplot(test_data, aes(x = x, y = perfect_linear)) + geom_point()

#Fit linear model
perfect_model <- lm(perfect_linear ~ x, data = test_data)

#Diagnostic plots
resid_panel(perfect_model)
```

Now you can then complete the exercise with the four other instances. How do the diagnostic plots 
look when the assumptions of a linear regression are violated?

```{r linear_regression_assumptions}

```

# Exercise 8B iv) - Transformations for linear regression